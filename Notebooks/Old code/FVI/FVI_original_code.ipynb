{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed13e3f4-0ca0-4584-99fd-1b24c619cf21",
   "metadata": {},
   "source": [
    "This code comes: https://github.com/hanbingyan/FVIOT/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4edf8d7a-2b21-4a40-afa1-3159ddfd74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(12345)\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "# check gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config\n",
    "MEM_SIZE = 3000\n",
    "BATCH_SIZE = 128\n",
    "DISCOUNT = 1.0\n",
    "N_INSTANCE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6d8b1-cb73-4321-8575-bceef17f2bfb",
   "metadata": {},
   "source": [
    "Utility functions for a more compact code and for the optimisation of the nerual networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af0eccd-6725-4242-b27f-667020e53863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch.nn as nn\n",
    "\n",
    "# def sinkhorn_knopp(mu, nu, C, reg, niter):\n",
    "#     K = np.exp(-C/C.max()/reg)\n",
    "#     u = np.ones((len(mu), ))\n",
    "#     for i in range(1, niter):\n",
    "#         v = nu/np.dot(K.T, u)\n",
    "#         u = mu/(np.dot(K, v))\n",
    "#     Pi = np.diag(u) @ K @ np.diag(v)\n",
    "#     return Pi\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition', ('time', 'x', 'y', 'value'))\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "def optimize_model(policy_net, memory, optimizer, Trunc_flag):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    values_batch = torch.stack(batch.value)\n",
    "    x_batch = torch.stack(batch.x)\n",
    "    y_batch = torch.stack(batch.y)\n",
    "    time_batch = torch.stack(batch.time)\n",
    "\n",
    "    left_values = policy_net(time_batch, x_batch, y_batch)\n",
    "\n",
    "    # # Compute the expected Q values\n",
    "    Loss_fn = nn.SmoothL1Loss()\n",
    "    # Loss_fn = nn.MSELoss()\n",
    "    loss = Loss_fn(left_values, values_batch)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if Trunc_flag:\n",
    "        for param in policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5008f3da-9a6a-49e4-aa09-8ab14e0b776e",
   "metadata": {},
   "source": [
    "The neural network at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04caccf1-804c-4832-bd22-cd0115d9b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nbimporter\n",
    "\n",
    "h = 8\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, T):\n",
    "        super(DQN, self).__init__()\n",
    "        self.T = T\n",
    "        self.linear1 = nn.Linear(x_dim+y_dim, h)\n",
    "        # self.linear1.weight.data.fill_(10.0)\n",
    "        # torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        # torch.nn.init.zeros_(self.linear1.weight)\n",
    "        # torch.nn.init.zeros_(self.linear1.bias)\n",
    "        # self.bn = nn.BatchNorm1d(h)\n",
    "        self.linear2 = nn.Linear(h, h)\n",
    "        # torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        # torch.nn.init.zeros_(self.linear2.bias)\n",
    "        # torch.nn.init.zeros_(self.linear2.weight)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.linear3 = nn.Linear(h, 1)\n",
    "\n",
    "        self.linear5 = nn.Linear(2, 1)\n",
    "        # torch.nn.init.zeros_(self.linear5.bias)\n",
    "        # torch.nn.init.zeros_(self.linear5.weight)\n",
    "        # torch.nn.init.xavier_uniform_(self.linear5.weight)\n",
    "        self.linear6 = nn.Linear(2, 1)\n",
    "        # torch.nn.init.zeros_(self.linear6.bias)\n",
    "\n",
    "    def forward(self, time, x, y):\n",
    "        state = torch.cat((x, y), dim=1)\n",
    "        state = torch.relu(self.linear1(state))\n",
    "        # state = self.bn(state)\n",
    "        state = torch.relu(self.linear2(state))\n",
    "        # state = self.dropout(state)\n",
    "        state = torch.sigmoid(self.linear3(state))\n",
    "        time_f2 = torch.cat((self.T - time, (self.T - time)**2), dim=1)\n",
    "        time_f1 =  self.linear5(time_f2)\n",
    "        time_f2 = self.linear6(time_f2)\n",
    "        return state*time_f1 + time_f2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5a484-11a1-499c-8eb0-eb1eee366357",
   "metadata": {},
   "source": [
    "The code to compute the Adapted Wasserstein distance (AW_2) between two brownian path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08fc842d-b1c8-4bd4-87e1-fe45a11a8664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance 0\n",
      "Last values 19.04200553894043\n",
      "Instance 1\n",
      "Last values 19.33392333984375\n",
      "Instance 2\n",
      "Last values 19.00780487060547\n",
      "Instance 3\n",
      "Last values 20.59442901611328\n",
      "Instance 4\n",
      "Last values 21.410417556762695\n",
      "Instance 5\n",
      "Last values 19.327674865722656\n",
      "Instance 6\n",
      "Last values 18.96123504638672\n",
      "Instance 7\n",
      "Last values 17.803863525390625\n",
      "Instance 8\n",
      "Last values 19.3609676361084\n",
      "Instance 9\n",
      "Last values 16.671588897705078\n",
      "All final value: [19.04200554 19.33392334 19.00780487 20.59442902 21.41041756 19.32767487\n",
      " 18.96123505 17.80386353 19.36096764 16.6715889 ]\n",
      "Final mean: 19.15139102935791\n",
      "Final std: 1.2380576047432714\n",
      "   Time Horizon  Mean Value   Std Dev  Avg Time per Iter\n",
      "0             8   19.151391  1.238058          40.074413\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import ot\n",
    "import time as Clock\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "start = Clock.time()\n",
    "\n",
    "####### One-dimensional case #########\n",
    "Trunc_flag = True\n",
    "N_INSTANCE = 10  # Run 10 iterations per time step\n",
    "\n",
    "def get_N_OPT(time):\n",
    "    if 1 <= time <= 5:\n",
    "        return 50\n",
    "    elif time == 6:\n",
    "        return 40\n",
    "    elif time == 7:\n",
    "        return 30\n",
    "    else:\n",
    "        return 20\n",
    "\n",
    "def get_sample_size(time):\n",
    "    if time == 15:\n",
    "        return 4000\n",
    "    elif time in [10, 9]:\n",
    "        return 3000\n",
    "    elif time in [7, 8]:\n",
    "        return 2500\n",
    "    else:\n",
    "        return 2000\n",
    "\n",
    "final_result = []\n",
    "time_horizon_list = list(range(8,9))\n",
    "summary_data = []\n",
    "in_sample_size = 10\n",
    "\n",
    "for time_horizon in time_horizon_list:\n",
    "    time_step_results = np.zeros(N_INSTANCE)\n",
    "    start_time = Clock.time()\n",
    "    for n_ins in range(N_INSTANCE):\n",
    "        x_dim = 1\n",
    "        y_dim = 1\n",
    "        x_vol = 1.0\n",
    "        y_vol = 0.5\n",
    "        x_init = 1.0\n",
    "        y_init = 2.0\n",
    "        \n",
    "        val_hist = np.zeros(time_horizon+1)\n",
    "        loss_hist = np.zeros(time_horizon+1)\n",
    "        \n",
    "        memory = Memory(MEM_SIZE)\n",
    "        policy_net = DQN(x_dim, y_dim, time_horizon).to(device)\n",
    "        target_net = DQN(x_dim, y_dim, time_horizon).to(device)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        target_net.eval()\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=1e-2)\n",
    "\n",
    "        for time in range(time_horizon, -1, -1):\n",
    "            N_OPT = get_N_OPT(time)\n",
    "            smp_size = get_sample_size(time)\n",
    "            \n",
    "            x_path_pool = torch.zeros(smp_size, time_horizon+1, x_dim, device=device)\n",
    "            y_path_pool = torch.zeros(smp_size, time_horizon+1, y_dim, device=device)\n",
    "            x_path_pool[:, 0, :] = x_init\n",
    "            y_path_pool[:, 0, :] = y_init\n",
    "\n",
    "            for smp_id in range(smp_size):\n",
    "                for t in range(1, time_horizon + 1):\n",
    "                    x_path_pool[smp_id, t, :] = x_path_pool[smp_id, t - 1, :] + x_vol * torch.randn(x_dim, device=device)\n",
    "                    y_path_pool[smp_id, t, :] = y_path_pool[smp_id, t - 1, :] + y_vol * torch.randn(y_dim, device=device)\n",
    "\n",
    "            for smp_id in range(smp_size):\n",
    "                x_mvn = MultivariateNormal(loc=x_path_pool[smp_id, time, :], covariance_matrix=torch.eye(x_dim, device=device)*x_vol**2)\n",
    "                y_mvn = MultivariateNormal(loc=y_path_pool[smp_id, time, :], covariance_matrix=torch.eye(y_dim, device=device)*y_vol**2)\n",
    "                next_x = x_mvn.sample((in_sample_size,))\n",
    "                next_y = y_mvn.sample((in_sample_size,))\n",
    "                \n",
    "                x_batch = torch.repeat_interleave(next_x, repeats=in_sample_size, dim=0)\n",
    "                y_batch = torch.tile(next_y, (in_sample_size, 1))\n",
    "                l2_mat = torch.sum((x_batch - y_batch)**2, dim=1)\n",
    "\n",
    "                if time == time_horizon:\n",
    "                    expected_v = 0.0\n",
    "                elif time == time_horizon-1:\n",
    "                    min_obj = l2_mat.reshape(in_sample_size, in_sample_size)\n",
    "                    expected_v = ot.emd2(np.ones(in_sample_size) / in_sample_size, np.ones(in_sample_size) / in_sample_size,\n",
    "                                         min_obj.detach().cpu().numpy())\n",
    "                else:\n",
    "                    val = target_net(torch.ones(x_batch.shape[0], 1, device=device)*(time+1.0), x_batch, y_batch).reshape(-1)\n",
    "                    min_obj = (l2_mat + DISCOUNT*val).reshape(in_sample_size, in_sample_size)\n",
    "                    expected_v = ot.emd2(np.ones(in_sample_size)/in_sample_size, np.ones(in_sample_size)/in_sample_size,\n",
    "                                         min_obj.detach().cpu().numpy())\n",
    "\n",
    "                memory.push(torch.tensor([time], dtype=torch.float32, device=device), x_path_pool[smp_id, time, :],\n",
    "                            y_path_pool[smp_id, time, :], torch.tensor([expected_v], device=device))\n",
    "\n",
    "            for opt_step in range(N_OPT):\n",
    "                loss = optimize_model(policy_net, memory, optimizer, Trunc_flag)\n",
    "                if Trunc_flag:\n",
    "                    with torch.no_grad():\n",
    "                        for param in policy_net.parameters():\n",
    "                            param.clamp_(-1.0, 1.0)\n",
    "                if loss:\n",
    "                    loss_hist[time] += loss.detach().cpu().item()\n",
    "\n",
    "            loss_hist[time] /= N_OPT\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            val = target_net(torch.ones(1, 1, device=device)*0.0, x_path_pool[0, 0, :].reshape(1, x_dim),\n",
    "                             y_path_pool[0, 0, :].reshape(1, y_dim)).reshape(-1)\n",
    "            val_hist[time] = val\n",
    "            memory.clear()\n",
    "\n",
    "        print(f'Instance {n_ins}')\n",
    "        print(f'Last values {val_hist[0]}')\n",
    "        time_step_results[n_ins] = val_hist[0]\n",
    "    \n",
    "    avg_time_per_iter = (Clock.time() - start_time) / N_INSTANCE\n",
    "    print(f'All final value: {time_step_results}')\n",
    "    print(f'Final mean: {time_step_results.mean()}')\n",
    "    print(f'Final std: {time_step_results.std()}')\n",
    "    summary_data.append([time_horizon, time_step_results.mean(), time_step_results.std(), avg_time_per_iter])\n",
    "    final_result.append(time_step_results.mean())\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data, columns=[\"Time Horizon\", \"Mean Value\", \"Std Dev\", \"Avg Time per Iter\"])\n",
    "print(summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c369717e",
   "metadata": {},
   "source": [
    "## SUPER LONG TO RUN DISCRETE CDE USING KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec2e2b-e9e2-4e7f-bb23-9b9de28706f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 4 Loss 3.4736964527724014e-05\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 3 Loss 3.8936922359466553\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 2 Loss 2.537625653743744\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 1 Loss 1.4277370071411133\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 0 Loss 0.048351617008447645\n",
      "Instance 0\n",
      "Last values 7.248353004455566\n",
      "Time step 4 Loss 0.03434914529090747\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 3 Loss 4.1678907012939455\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 2 Loss 2.3007030630111696\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 1 Loss 0.6179903799295425\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 0 Loss 0.14421652406454086\n",
      "Instance 1\n",
      "Last values 7.383070945739746\n",
      "Time step 4 Loss 0.018641373459249735\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 3 Loss 3.7531351900100707\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 2 Loss 1.9731627488136292\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 1 Loss 0.6234582430124282\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 0 Loss 0.185251906812191\n",
      "Instance 2\n",
      "Last values 7.572972297668457\n",
      "Time step 4 Loss 0.028126418483152518\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 3 Loss 3.882399249076843\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 2 Loss 2.448329899311066\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 1 Loss 1.2063075375556946\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 0 Loss 0.12136880502104759\n",
      "Instance 3\n",
      "Last values 6.131731033325195\n",
      "Time step 4 Loss 0.018912516813288677\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 3 Loss 4.079599418640137\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 2 Loss 2.5022585511207582\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 1 Loss 0.963834410905838\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 0 Loss 0.07581349559128285\n",
      "Instance 4\n",
      "Last values 7.015209197998047\n",
      "Time step 4 Loss 6.5843834300949315e-06\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 3 Loss 3.7276201581954957\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 2 Loss 2.1913543462753298\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 1 Loss 0.7133736693859101\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 0 Loss 0.2066568547487259\n",
      "Instance 5\n",
      "Last values 7.035223007202148\n",
      "Time step 4 Loss 0.061662511313334105\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 3 Loss 3.639169692993164\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 2 Loss 2.469142417907715\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 1 Loss 0.6043326646089554\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 0 Loss 0.16664200872182847\n",
      "Instance 6\n",
      "Last values 7.050169944763184\n",
      "Time step 4 Loss 0.13441685102880002\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 3 Loss 3.722045211791992\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 2 Loss 1.6667962396144866\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 1 Loss 0.702357959151268\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 0 Loss 0.18741510719060897\n",
      "Instance 7\n",
      "Last values 7.37906551361084\n",
      "Time step 4 Loss 0.0019123231617413695\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 3 Loss 3.675914001464844\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 2 Loss 2.1061537075042724\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 1 Loss 0.7713468801975251\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 0 Loss 0.18381349608302117\n",
      "Instance 8\n",
      "Last values 7.5215654373168945\n",
      "Time step 4 Loss 0.0053255060125097665\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 3 Loss 4.124874081611633\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 2 Loss 2.449748363494873\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 1 Loss 0.7141401708126068\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "Time step 0 Loss 0.20417029052972793\n",
      "Instance 9\n",
      "Last values 7.616240501403809\n",
      "All final value: [7.248353   7.38307095 7.5729723  6.13173103 7.0152092  7.03522301\n",
      " 7.05016994 7.37906551 7.52156544 7.6162405 ]\n",
      "Final mean: 7.195360088348389\n",
      "Final std: 0.4133300985125363\n",
      "Average time for one instance: 7472.819194889069\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import ot\n",
    "import time as Clock\n",
    "\n",
    "start = Clock.time()\n",
    "\n",
    "####### One-dimensional case #########\n",
    "# with parameter constraint\n",
    "Trunc_flag = True\n",
    "# No. of gradient descent steps (G)\n",
    "N_OPT = 50\n",
    "# No. of sample paths (N)\n",
    "smp_size = 2000\n",
    "# Sample size for empirical OT (B)\n",
    "in_sample_size = 50\n",
    "\n",
    "time_horizon = 4\n",
    "x_dim = 1\n",
    "y_dim = 1\n",
    "x_vol = 1.0\n",
    "y_vol = 0.5\n",
    "x_init = 1.0\n",
    "y_init = 2.0\n",
    "\n",
    "\n",
    "final_result = np.zeros(N_INSTANCE)\n",
    "\n",
    "for n_ins in range(N_INSTANCE):\n",
    "\n",
    "    val_hist = np.zeros(time_horizon+1)\n",
    "    loss_hist = np.zeros(time_horizon+1)\n",
    "\n",
    "    memory = Memory(MEM_SIZE)\n",
    "    policy_net = DQN(x_dim, y_dim, time_horizon).to(device)\n",
    "    target_net = DQN(x_dim, y_dim, time_horizon).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    # optimizer = optim.SGD(policy_net.parameters(), lr=0.1, momentum=0.9)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-2) # weight_decay=1e-3)\n",
    "\n",
    "    x_path_pool = torch.zeros(smp_size, time_horizon+1, x_dim, device=device)\n",
    "    y_path_pool = torch.zeros(smp_size, time_horizon+1, y_dim, device=device)\n",
    "    x_path_pool[:, 0, :] = x_init\n",
    "    y_path_pool[:, 0, :] = y_init\n",
    "\n",
    "    for smp_id in range(smp_size):\n",
    "        # sample many paths in advance\n",
    "        for t in range(1, time_horizon + 1):\n",
    "            x_path_pool[smp_id, t, :] = x_path_pool[smp_id, t - 1, :] + x_vol * torch.randn(x_dim, device=device)\n",
    "            y_path_pool[smp_id, t, :] = y_path_pool[smp_id, t - 1, :] + y_vol * torch.randn(y_dim, device=device)\n",
    "\n",
    "    for time in range(time_horizon, -1, -1):\n",
    "\n",
    "        for smp_id in range(smp_size):\n",
    "\n",
    "            if time < time_horizon:\n",
    "                if smp_id % 100 ==0:\n",
    "                    print(smp_id)\n",
    "                x_mvn = MultivariateNormal(loc=x_path_pool[smp_id, time, :], covariance_matrix=torch.eye(x_dim, device=device)*x_vol**2)\n",
    "                y_mvn = MultivariateNormal(loc=y_path_pool[smp_id, time, :], covariance_matrix=torch.eye(y_dim, device=device)*y_vol**2)\n",
    "                # Draw 400 samples for x and y from the respective Gaussian distributions\n",
    "                num_samples = 150\n",
    "                sampled_x = x_mvn.sample((num_samples,))  # shape: (400, x_dim)\n",
    "                sampled_y = y_mvn.sample((num_samples,))  # shape: (400, y_dim)\n",
    "\n",
    "                # Transfer to CPU and convert to numpy arrays (needed for sklearn's KMeans)\n",
    "                sampled_x_np = sampled_x.detach().cpu().numpy()\n",
    "                sampled_y_np = sampled_y.detach().cpu().numpy()\n",
    "\n",
    "                # Cluster the x samples\n",
    "                kmeans_x = KMeans(n_clusters=in_sample_size, n_init=10).fit(sampled_x_np)\n",
    "                centers_x = torch.tensor(kmeans_x.cluster_centers_, device=device, dtype=sampled_x.dtype)\n",
    "                # Compute the cluster weights (nonuniform probabilities)\n",
    "                weights_x_np = np.bincount(kmeans_x.labels_, minlength=in_sample_size).astype(np.float32) / num_samples\n",
    "\n",
    "                # Cluster the y samples\n",
    "                kmeans_y = KMeans(n_clusters=in_sample_size, n_init=10).fit(sampled_y_np)\n",
    "                centers_y = torch.tensor(kmeans_y.cluster_centers_, device=device, dtype=sampled_y.dtype)\n",
    "                weights_y_np = np.bincount(kmeans_y.labels_, minlength=in_sample_size).astype(np.float32) / num_samples\n",
    "\n",
    "                # Compute the squared Euclidean cost matrix between cluster centers\n",
    "                cost_matrix = torch.cdist(centers_x, centers_y, p=2)**2\n",
    "\n",
    "            # Now, depending on the time step, compute expected value via optimal transport\n",
    "            if time == time_horizon:\n",
    "                expected_v = 0.0\n",
    "            elif time == time_horizon - 1:\n",
    "                # For the final step, use only the distance cost\n",
    "                expected_v = ot.emd2(weights_x_np, weights_y_np, cost_matrix.detach().cpu().numpy())\n",
    "            else:\n",
    "                # For intermediate times, incorporate the continuation value from the target network.\n",
    "                # Compute the pairwise V-values for each (center_x, center_y) pair.\n",
    "                # Create a grid of centers\n",
    "                X_grid = centers_x.unsqueeze(1).expand(in_sample_size, in_sample_size, x_dim)\n",
    "                Y_grid = centers_y.unsqueeze(0).expand(in_sample_size, in_sample_size, y_dim)\n",
    "                time_tensor = torch.ones((in_sample_size, in_sample_size, 1), device=device) * (time + 1.0)\n",
    "                \n",
    "                # Reshape for batch evaluation\n",
    "                x_input = X_grid.reshape(-1, x_dim)\n",
    "                y_input = Y_grid.reshape(-1, y_dim)\n",
    "                time_input = time_tensor.reshape(-1, 1)\n",
    "                val = target_net(time_input, x_input, y_input).reshape(in_sample_size, in_sample_size)\n",
    "                \n",
    "                # Add the discounted value function to the cost matrix\n",
    "                cost_matrix = cost_matrix + DISCOUNT * val\n",
    "                expected_v = ot.emd2(weights_x_np, weights_y_np, cost_matrix.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "            memory.push(torch.tensor([time], dtype=torch.float32, device=device), x_path_pool[smp_id, time, :],\n",
    "                        y_path_pool[smp_id, time, :], torch.tensor([expected_v], device=device))\n",
    "\n",
    "        # Optimize at time t\n",
    "        for opt_step in range(N_OPT):\n",
    "            loss = optimize_model(policy_net, memory, optimizer, Trunc_flag)\n",
    "            if Trunc_flag:\n",
    "                with torch.no_grad():\n",
    "                    for param in policy_net.parameters():\n",
    "                        ## param.add_(torch.randn(param.size(), device=device)/50)\n",
    "                        param.clamp_(-1.0, 1.0)\n",
    "            if loss:\n",
    "                loss_hist[time] += loss.detach().cpu().item()\n",
    "\n",
    "\n",
    "        loss_hist[time] /= N_OPT\n",
    "\n",
    "        # update target network\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        # test initial value\n",
    "        val = target_net(torch.ones(1, 1, device=device)*0.0, x_path_pool[0, 0, :].reshape(1, x_dim),\n",
    "                         y_path_pool[0, 0, :].reshape(1, y_dim)).reshape(-1)\n",
    "        val_hist[time] = val\n",
    "\n",
    "        # empty memory\n",
    "        memory.clear()\n",
    "        print('Time step', time, 'Loss', loss_hist[time])\n",
    "\n",
    "        # print('Shift vector in the last layer:', target_net.linear3.bias.sum().item())\n",
    "\n",
    "\n",
    "    # for name, param in target_net.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name, param.data)\n",
    "\n",
    "\n",
    "    print('Instance', n_ins)\n",
    "    # print('Time elapsed', end - start)\n",
    "    print('Last values', val_hist[0])\n",
    "    final_result[n_ins] = val_hist[0]\n",
    "\n",
    "print('All final value:', final_result)\n",
    "print('Final mean:', final_result.mean())\n",
    "print('Final std:', final_result.std())\n",
    "end = Clock.time()\n",
    "print('Average time for one instance:', (end-start)/N_INSTANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7dbd0b-7d50-4b20-bb00-b0a268973091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
