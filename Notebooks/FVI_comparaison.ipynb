{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(12345)\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(12345)\n",
    "# check gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Config\n",
    "MEM_SIZE = 3000\n",
    "BATCH_SIZE = 128\n",
    "DISCOUNT = 1.0\n",
    "N_INSTANCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch.nn as nn\n",
    "\n",
    "# def sinkhorn_knopp(mu, nu, C, reg, niter):\n",
    "#     K = np.exp(-C/C.max()/reg)\n",
    "#     u = np.ones((len(mu), ))\n",
    "#     for i in range(1, niter):\n",
    "#         v = nu/np.dot(K.T, u)\n",
    "#         u = mu/(np.dot(K, v))\n",
    "#     Pi = np.diag(u) @ K @ np.diag(v)\n",
    "#     return Pi\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition', ('time', 'x', 'y', 'value'))\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def clear(self):\n",
    "        self.memory.clear()\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "def optimize_model(policy_net, memory, optimizer, Trunc_flag):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    values_batch = torch.stack(batch.value)\n",
    "    x_batch = torch.stack(batch.x)\n",
    "    y_batch = torch.stack(batch.y)\n",
    "    time_batch = torch.stack(batch.time)\n",
    "\n",
    "    left_values = policy_net(time_batch, x_batch, y_batch)\n",
    "\n",
    "    # # Compute the expected Q values\n",
    "    Loss_fn = nn.SmoothL1Loss()\n",
    "    # Loss_fn = nn.MSELoss()\n",
    "    loss = Loss_fn(left_values, values_batch)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    if Trunc_flag:\n",
    "        for param in policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "h = 8\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, x_dim, y_dim, T):\n",
    "        super(DQN, self).__init__()\n",
    "        self.T = T\n",
    "        self.linear1 = nn.Linear(x_dim+y_dim, h)\n",
    "        # self.linear1.weight.data.fill_(10.0)\n",
    "        # torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        # torch.nn.init.zeros_(self.linear1.weight)\n",
    "        # torch.nn.init.zeros_(self.linear1.bias)\n",
    "        # self.bn = nn.BatchNorm1d(h)\n",
    "        self.linear2 = nn.Linear(h, h)\n",
    "        # torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        # torch.nn.init.zeros_(self.linear2.bias)\n",
    "        # torch.nn.init.zeros_(self.linear2.weight)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.linear3 = nn.Linear(h, 1)\n",
    "\n",
    "        self.linear5 = nn.Linear(2, 1)\n",
    "        # torch.nn.init.zeros_(self.linear5.bias)\n",
    "        # torch.nn.init.zeros_(self.linear5.weight)\n",
    "        # torch.nn.init.xavier_uniform_(self.linear5.weight)\n",
    "        self.linear6 = nn.Linear(2, 1)\n",
    "        # torch.nn.init.zeros_(self.linear6.bias)\n",
    "\n",
    "    def forward(self, time, x, y):\n",
    "        state = torch.cat((x, y), dim=1)\n",
    "        state = torch.relu(self.linear1(state))\n",
    "        # state = self.bn(state)\n",
    "        state = torch.relu(self.linear2(state))\n",
    "        # state = self.dropout(state)\n",
    "        state = torch.sigmoid(self.linear3(state))\n",
    "        time_f2 = torch.cat((self.T - time, (self.T - time)**2), dim=1)\n",
    "        time_f1 =  self.linear5(time_f2)\n",
    "        time_f2 = self.linear6(time_f2)\n",
    "        return state*time_f1 + time_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 30 Loss 0.028862596582621337\n",
      "Time step 29 Loss 148.23358383178712\n",
      "Time step 28 Loss 147.13817672729493\n",
      "Time step 27 Loss 134.0324794769287\n",
      "Time step 26 Loss 125.68193435668945\n",
      "Time step 25 Loss 115.61860084533691\n",
      "Time step 24 Loss 106.17122344970703\n",
      "Time step 23 Loss 98.48856773376465\n",
      "Time step 22 Loss 95.83695755004882\n",
      "Time step 21 Loss 86.10469131469726\n",
      "Time step 20 Loss 76.86632423400879\n",
      "Time step 19 Loss 69.2186845779419\n",
      "Time step 18 Loss 65.00307388305664\n",
      "Time step 17 Loss 59.01581974029541\n",
      "Time step 16 Loss 56.81620502471924\n",
      "Time step 15 Loss 51.87574920654297\n",
      "Time step 14 Loss 47.453945541381835\n",
      "Time step 13 Loss 41.145558738708495\n",
      "Time step 12 Loss 38.36333541870117\n",
      "Time step 11 Loss 34.91278524398804\n",
      "Time step 10 Loss 35.88857946395874\n",
      "Time step 9 Loss 31.429337406158446\n",
      "Time step 8 Loss 25.88802366256714\n",
      "Time step 7 Loss 24.05263195037842\n",
      "Time step 6 Loss 20.67664556503296\n",
      "Time step 5 Loss 19.36995325088501\n",
      "Time step 4 Loss 18.87670793533325\n",
      "Time step 3 Loss 18.75961217880249\n",
      "Time step 2 Loss 15.676143217086793\n",
      "Time step 1 Loss 12.198186874389648\n",
      "Time step 0 Loss 8.619242203235626\n",
      "Instance 0\n",
      "Last values 401.4665222167969\n",
      "Time step 30 Loss 0.20499805584549904\n",
      "Time step 29 Loss 149.26436538696288\n",
      "Time step 28 Loss 143.4102611541748\n",
      "Time step 27 Loss 132.30893669128417\n",
      "Time step 26 Loss 125.40372772216797\n",
      "Time step 25 Loss 117.18000068664551\n",
      "Time step 24 Loss 109.59657592773438\n",
      "Time step 23 Loss 106.73932838439941\n",
      "Time step 22 Loss 96.81159324645996\n",
      "Time step 21 Loss 94.47871475219726\n",
      "Time step 20 Loss 84.14392318725587\n",
      "Time step 19 Loss 73.48855152130128\n",
      "Time step 18 Loss 69.06816596984864\n",
      "Time step 17 Loss 69.38710613250733\n",
      "Time step 16 Loss 60.23883686065674\n",
      "Time step 15 Loss 56.005642890930176\n",
      "Time step 14 Loss 47.442908668518065\n",
      "Time step 13 Loss 47.446357345581056\n",
      "Time step 12 Loss 44.61921377182007\n",
      "Time step 11 Loss 37.694744205474855\n",
      "Time step 10 Loss 36.08437995910644\n",
      "Time step 9 Loss 32.685866355895996\n",
      "Time step 8 Loss 29.27565517425537\n",
      "Time step 7 Loss 26.212610054016114\n",
      "Time step 6 Loss 25.61834955215454\n",
      "Time step 5 Loss 20.391471576690673\n",
      "Time step 4 Loss 19.50433392524719\n",
      "Time step 3 Loss 18.159088945388795\n",
      "Time step 2 Loss 15.171850681304932\n",
      "Time step 1 Loss 14.241502285003662\n",
      "Time step 0 Loss 10.163424503803252\n",
      "Instance 1\n",
      "Last values 394.041015625\n",
      "Time step 30 Loss 0.01959826720703859\n",
      "Time step 29 Loss 145.4868293762207\n",
      "Time step 28 Loss 140.44895248413087\n",
      "Time step 27 Loss 129.4363079071045\n",
      "Time step 26 Loss 115.93374519348144\n",
      "Time step 25 Loss 108.46778717041016\n",
      "Time step 24 Loss 102.81149635314941\n",
      "Time step 23 Loss 91.71884193420411\n",
      "Time step 22 Loss 83.32458305358887\n",
      "Time step 21 Loss 77.06605987548828\n",
      "Time step 20 Loss 74.5513292312622\n",
      "Time step 19 Loss 72.67069759368897\n",
      "Time step 18 Loss 63.010124397277835\n",
      "Time step 17 Loss 58.87555294036865\n",
      "Time step 16 Loss 56.11284255981445\n",
      "Time step 15 Loss 50.668890953063965\n",
      "Time step 14 Loss 49.36670894622803\n",
      "Time step 13 Loss 46.478805732727054\n",
      "Time step 12 Loss 40.63693227767944\n",
      "Time step 11 Loss 34.97929124832153\n",
      "Time step 10 Loss 33.3089319229126\n",
      "Time step 9 Loss 27.530485343933105\n",
      "Time step 8 Loss 27.727416610717775\n",
      "Time step 7 Loss 23.60307321548462\n",
      "Time step 6 Loss 20.31451654434204\n",
      "Time step 5 Loss 19.05781545639038\n",
      "Time step 4 Loss 16.43176531791687\n",
      "Time step 3 Loss 17.292029047012328\n",
      "Time step 2 Loss 14.431056213378906\n",
      "Time step 1 Loss 14.483495235443115\n",
      "Time step 0 Loss 7.862466657161713\n",
      "Instance 2\n",
      "Last values 392.64337158203125\n",
      "Time step 30 Loss 0.012680559628643095\n",
      "Time step 29 Loss 136.4770606994629\n",
      "Time step 28 Loss 135.68603706359863\n",
      "Time step 27 Loss 122.88308792114258\n",
      "Time step 26 Loss 114.96313667297363\n",
      "Time step 25 Loss 108.51962661743164\n",
      "Time step 24 Loss 105.64043045043945\n",
      "Time step 23 Loss 97.77318878173828\n",
      "Time step 22 Loss 94.8244239807129\n",
      "Time step 21 Loss 92.36773643493652\n",
      "Time step 20 Loss 82.45212173461914\n",
      "Time step 19 Loss 77.89772911071778\n",
      "Time step 18 Loss 72.94407749176025\n",
      "Time step 17 Loss 66.66193580627441\n",
      "Time step 16 Loss 60.997727394104004\n",
      "Time step 15 Loss 56.80121784210205\n",
      "Time step 14 Loss 52.93300228118896\n",
      "Time step 13 Loss 47.218772315979\n",
      "Time step 12 Loss 45.70357112884521\n",
      "Time step 11 Loss 40.352467155456544\n",
      "Time step 10 Loss 39.012396812438965\n",
      "Time step 9 Loss 33.108090782165526\n",
      "Time step 8 Loss 32.82529697418213\n",
      "Time step 7 Loss 26.64540843963623\n",
      "Time step 6 Loss 25.104509830474854\n",
      "Time step 5 Loss 23.01491961479187\n",
      "Time step 4 Loss 19.44722275733948\n",
      "Time step 3 Loss 19.046088790893556\n",
      "Time step 2 Loss 17.162816429138182\n",
      "Time step 1 Loss 15.385703516006469\n",
      "Time step 0 Loss 11.061431872844697\n",
      "Instance 3\n",
      "Last values 486.24029541015625\n",
      "Time step 30 Loss 0.01738242697902024\n",
      "Time step 29 Loss 141.42308349609374\n",
      "Time step 28 Loss 139.5810001373291\n",
      "Time step 27 Loss 129.27510261535645\n",
      "Time step 26 Loss 125.56715126037598\n",
      "Time step 25 Loss 112.12802963256836\n",
      "Time step 24 Loss 103.31784133911133\n",
      "Time step 23 Loss 101.9801456451416\n",
      "Time step 22 Loss 88.90970993041992\n",
      "Time step 21 Loss 90.60604553222656\n",
      "Time step 20 Loss 74.66361236572266\n",
      "Time step 19 Loss 72.3553253173828\n",
      "Time step 18 Loss 64.16033840179443\n",
      "Time step 17 Loss 64.18277683258057\n",
      "Time step 16 Loss 62.65368022918701\n",
      "Time step 15 Loss 51.027025032043454\n",
      "Time step 14 Loss 47.800011253356935\n",
      "Time step 13 Loss 40.26917266845703\n",
      "Time step 12 Loss 42.693258380889894\n",
      "Time step 11 Loss 34.540669441223145\n",
      "Time step 10 Loss 36.42054138183594\n",
      "Time step 9 Loss 30.719828510284422\n",
      "Time step 8 Loss 28.398671245574953\n",
      "Time step 7 Loss 24.96727294921875\n",
      "Time step 6 Loss 21.331739711761475\n",
      "Time step 5 Loss 20.290113544464113\n",
      "Time step 4 Loss 20.12202305793762\n",
      "Time step 3 Loss 17.449526977539062\n",
      "Time step 2 Loss 16.060687875747682\n",
      "Time step 1 Loss 12.544609975814819\n",
      "Time step 0 Loss 10.616324788331985\n",
      "Instance 4\n",
      "Last values 411.5376892089844\n",
      "Time step 30 Loss 0.2939077392220497\n",
      "Time step 29 Loss 144.67585792541504\n",
      "Time step 28 Loss 141.06530265808107\n",
      "Time step 27 Loss 128.75002822875976\n",
      "Time step 26 Loss 122.13959159851075\n",
      "Time step 25 Loss 114.00353164672852\n",
      "Time step 24 Loss 101.932275390625\n",
      "Time step 23 Loss 97.40010604858398\n",
      "Time step 22 Loss 92.82809104919434\n",
      "Time step 21 Loss 87.49061698913575\n",
      "Time step 20 Loss 74.87857704162597\n",
      "Time step 19 Loss 69.5164249420166\n",
      "Time step 18 Loss 64.46103210449219\n",
      "Time step 17 Loss 57.0177282333374\n",
      "Time step 16 Loss 54.55537109375\n",
      "Time step 15 Loss 50.73017063140869\n",
      "Time step 14 Loss 45.62929134368896\n",
      "Time step 13 Loss 44.577620315551755\n",
      "Time step 12 Loss 41.69415044784546\n",
      "Time step 11 Loss 39.344415950775144\n",
      "Time step 10 Loss 35.08401079177857\n",
      "Time step 9 Loss 31.94704008102417\n",
      "Time step 8 Loss 28.838681793212892\n",
      "Time step 7 Loss 26.05148878097534\n",
      "Time step 6 Loss 22.0092924118042\n",
      "Time step 5 Loss 20.584066009521486\n",
      "Time step 4 Loss 20.533685779571535\n",
      "Time step 3 Loss 18.73352508544922\n",
      "Time step 2 Loss 16.763424348831176\n",
      "Time step 1 Loss 14.863459634780884\n",
      "Time step 0 Loss 7.416480302810669\n",
      "Instance 5\n",
      "Last values 416.1214599609375\n",
      "Time step 30 Loss 0.16095030978322028\n",
      "Time step 29 Loss 137.84566802978514\n",
      "Time step 28 Loss 132.93826217651366\n",
      "Time step 27 Loss 120.74091262817383\n",
      "Time step 26 Loss 123.77512130737304\n",
      "Time step 25 Loss 113.78609237670898\n",
      "Time step 24 Loss 100.99812622070313\n",
      "Time step 23 Loss 90.84540367126465\n",
      "Time step 22 Loss 82.12104263305665\n",
      "Time step 21 Loss 81.48365173339843\n",
      "Time step 20 Loss 73.79535598754883\n",
      "Time step 19 Loss 68.48499736785888\n",
      "Time step 18 Loss 60.59701232910156\n",
      "Time step 17 Loss 54.72153186798096\n",
      "Time step 16 Loss 48.74981899261475\n",
      "Time step 15 Loss 47.838700675964354\n",
      "Time step 14 Loss 44.98853225708008\n",
      "Time step 13 Loss 39.15965614318848\n",
      "Time step 12 Loss 38.06058740615845\n",
      "Time step 11 Loss 34.17144947052002\n",
      "Time step 10 Loss 31.87238988876343\n",
      "Time step 9 Loss 31.35049114227295\n",
      "Time step 8 Loss 25.03903579711914\n",
      "Time step 7 Loss 23.689593696594237\n",
      "Time step 6 Loss 21.446342945098877\n",
      "Time step 5 Loss 20.391189336776733\n",
      "Time step 4 Loss 18.358587121963502\n",
      "Time step 3 Loss 17.23218035697937\n",
      "Time step 2 Loss 15.40939040184021\n",
      "Time step 1 Loss 10.258704280853271\n",
      "Time step 0 Loss 7.032230353355407\n",
      "Instance 6\n",
      "Last values 413.26898193359375\n",
      "Time step 30 Loss 0.0007201164178695762\n",
      "Time step 29 Loss 149.66512718200684\n",
      "Time step 28 Loss 140.06359481811523\n",
      "Time step 27 Loss 133.24618339538574\n",
      "Time step 26 Loss 124.83657150268554\n",
      "Time step 25 Loss 120.89936790466308\n",
      "Time step 24 Loss 112.63782196044922\n",
      "Time step 23 Loss 107.51006164550782\n",
      "Time step 22 Loss 95.80808372497559\n",
      "Time step 21 Loss 90.4996940612793\n",
      "Time step 20 Loss 82.51954307556153\n",
      "Time step 19 Loss 72.39805355072022\n",
      "Time step 18 Loss 65.48721809387207\n",
      "Time step 17 Loss 67.36605434417724\n",
      "Time step 16 Loss 61.143923377990724\n",
      "Time step 15 Loss 57.24382266998291\n",
      "Time step 14 Loss 50.079019927978514\n",
      "Time step 13 Loss 44.67213153839111\n",
      "Time step 12 Loss 40.5156928062439\n",
      "Time step 11 Loss 38.99881496429443\n",
      "Time step 10 Loss 34.08386745452881\n",
      "Time step 9 Loss 30.345763301849367\n",
      "Time step 8 Loss 26.636503791809083\n",
      "Time step 7 Loss 25.14596290588379\n",
      "Time step 6 Loss 23.154604339599608\n",
      "Time step 5 Loss 22.50293674468994\n",
      "Time step 4 Loss 19.55010304450989\n",
      "Time step 3 Loss 17.256184101104736\n",
      "Time step 2 Loss 15.878367471694947\n",
      "Time step 1 Loss 12.556328582763673\n",
      "Time step 0 Loss 5.9549745440483095\n",
      "Instance 7\n",
      "Last values 375.74237060546875\n",
      "Time step 30 Loss 0.000975871416812879\n",
      "Time step 29 Loss 146.42907371520997\n",
      "Time step 28 Loss 137.37737541198732\n",
      "Time step 27 Loss 129.08335762023927\n",
      "Time step 26 Loss 119.8979866027832\n",
      "Time step 25 Loss 110.05462493896485\n",
      "Time step 24 Loss 102.27043762207032\n",
      "Time step 23 Loss 93.15344200134277\n",
      "Time step 22 Loss 89.62831840515136\n",
      "Time step 21 Loss 83.36912956237794\n",
      "Time step 20 Loss 78.39272937774658\n",
      "Time step 19 Loss 70.89266662597656\n",
      "Time step 18 Loss 62.959475708007815\n",
      "Time step 17 Loss 63.963591957092284\n",
      "Time step 16 Loss 55.60354385375977\n",
      "Time step 15 Loss 51.56399021148682\n",
      "Time step 14 Loss 45.01307830810547\n",
      "Time step 13 Loss 42.44225578308105\n",
      "Time step 12 Loss 36.3951455116272\n",
      "Time step 11 Loss 34.86693601608276\n",
      "Time step 10 Loss 30.66359853744507\n",
      "Time step 9 Loss 27.964387702941895\n",
      "Time step 8 Loss 25.322215843200684\n",
      "Time step 7 Loss 23.18249092102051\n",
      "Time step 6 Loss 23.396857070922852\n",
      "Time step 5 Loss 19.71101098060608\n",
      "Time step 4 Loss 18.64550199508667\n",
      "Time step 3 Loss 17.340849208831788\n",
      "Time step 2 Loss 16.10674443244934\n",
      "Time step 1 Loss 13.781584739685059\n",
      "Time step 0 Loss 5.323913812637329\n",
      "Instance 8\n",
      "Last values 399.2943420410156\n",
      "Time step 30 Loss 0.05031623411923647\n",
      "Time step 29 Loss 143.0732593536377\n",
      "Time step 28 Loss 130.68695335388185\n",
      "Time step 27 Loss 129.7782241821289\n",
      "Time step 26 Loss 114.47886772155762\n",
      "Time step 25 Loss 111.1498291015625\n",
      "Time step 24 Loss 104.95555992126465\n",
      "Time step 23 Loss 94.63082313537598\n",
      "Time step 22 Loss 91.88924255371094\n",
      "Time step 21 Loss 79.68598194122315\n",
      "Time step 20 Loss 73.7882562637329\n",
      "Time step 19 Loss 71.94314918518066\n",
      "Time step 18 Loss 60.85510959625244\n",
      "Time step 17 Loss 64.59300136566162\n",
      "Time step 16 Loss 54.41000747680664\n",
      "Time step 15 Loss 50.6240852355957\n",
      "Time step 14 Loss 45.86045112609863\n",
      "Time step 13 Loss 44.13522777557373\n",
      "Time step 12 Loss 39.50686874389648\n",
      "Time step 11 Loss 36.02644863128662\n",
      "Time step 10 Loss 31.052374172210694\n",
      "Time step 9 Loss 29.155013465881346\n",
      "Time step 8 Loss 27.57974519729614\n",
      "Time step 7 Loss 22.741219806671143\n",
      "Time step 6 Loss 21.016652870178223\n",
      "Time step 5 Loss 17.424197340011595\n",
      "Time step 4 Loss 16.562741470336913\n",
      "Time step 3 Loss 16.379367971420287\n",
      "Time step 2 Loss 12.780320262908935\n",
      "Time step 1 Loss 11.801343965530396\n",
      "Time step 0 Loss 7.979154825210571\n",
      "Instance 9\n",
      "Last values 390.6426696777344\n",
      "All final value: [401.46652222 394.04101562 392.64337158 486.24029541 411.53768921\n",
      " 416.12145996 413.26898193 375.74237061 399.29434204 390.64266968]\n",
      "Final mean: 408.09987182617186\n",
      "Final std: 28.493271614834907\n",
      "Average time for one instance: 143.34788677692413\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import ot\n",
    "import time as Clock\n",
    "\n",
    "start = Clock.time()\n",
    "\n",
    "####### One-dimensional case #########\n",
    "# with parameter constraint\n",
    "Trunc_flag = True\n",
    "# No. of gradient descent steps (G)\n",
    "N_OPT = 20\n",
    "# No. of sample paths (N)\n",
    "smp_size = 2000\n",
    "# Sample size for empirical OT (B)\n",
    "in_sample_size = 50\n",
    "\n",
    "time_horizon = 30\n",
    "x_dim = 1\n",
    "y_dim = 1\n",
    "x_vol = 1.0\n",
    "y_vol = 2\n",
    "x_init = 0.0\n",
    "y_init = 0.0\n",
    "\n",
    "\n",
    "###### Multidimensional case #########\n",
    "## no parameter constraint\n",
    "# Trunc_flag = False\n",
    "# time_horizon = 5\n",
    "# x_dim = 5\n",
    "# y_dim = 5\n",
    "# x_vol = 1.1\n",
    "# y_vol = 0.1\n",
    "# x_init = 1.0\n",
    "# y_init = 2.0\n",
    "# N_OPT = 400\n",
    "# smp_size = 4000\n",
    "# in_sample_size = 300\n",
    "\n",
    "\n",
    "final_result = np.zeros(N_INSTANCE)\n",
    "\n",
    "for n_ins in range(N_INSTANCE):\n",
    "\n",
    "    val_hist = np.zeros(time_horizon+1)\n",
    "    loss_hist = np.zeros(time_horizon+1)\n",
    "\n",
    "    memory = Memory(MEM_SIZE)\n",
    "    policy_net = DQN(x_dim, y_dim, time_horizon).to(device)\n",
    "    target_net = DQN(x_dim, y_dim, time_horizon).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    # optimizer = optim.SGD(policy_net.parameters(), lr=0.1, momentum=0.9)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-2) # weight_decay=1e-3)\n",
    "\n",
    "    x_path_pool = torch.zeros(smp_size, time_horizon+1, x_dim, device=device)\n",
    "    y_path_pool = torch.zeros(smp_size, time_horizon+1, y_dim, device=device)\n",
    "    x_path_pool[:, 0, :] = x_init\n",
    "    y_path_pool[:, 0, :] = y_init\n",
    "\n",
    "    for smp_id in range(smp_size):\n",
    "        # sample many paths in advance\n",
    "        for t in range(1, time_horizon + 1):\n",
    "            x_path_pool[smp_id, t, :] = x_path_pool[smp_id, t - 1, :] + x_vol * torch.randn(x_dim, device=device)\n",
    "            y_path_pool[smp_id, t, :] = y_path_pool[smp_id, t - 1, :] + y_vol * torch.randn(y_dim, device=device)\n",
    "\n",
    "    for time in range(time_horizon, -1, -1):\n",
    "\n",
    "        for smp_id in range(smp_size):\n",
    "            x_mvn = MultivariateNormal(loc=x_path_pool[smp_id, time, :], covariance_matrix=torch.eye(x_dim, device=device)*x_vol**2)\n",
    "            y_mvn = MultivariateNormal(loc=y_path_pool[smp_id, time, :], covariance_matrix=torch.eye(y_dim, device=device)*y_vol**2)\n",
    "            next_x = x_mvn.sample((in_sample_size,))\n",
    "            next_y = y_mvn.sample((in_sample_size,))\n",
    "\n",
    "            x_batch = torch.repeat_interleave(next_x, repeats=in_sample_size, dim=0)\n",
    "            y_batch = torch.tile(next_y, (in_sample_size, 1))\n",
    "            l2_mat = torch.sum((x_batch - y_batch)**2, dim=1)\n",
    "\n",
    "            if time == time_horizon:\n",
    "                expected_v = 0.0\n",
    "            elif time == time_horizon-1:\n",
    "                min_obj = l2_mat.reshape(in_sample_size, in_sample_size)\n",
    "                expected_v = ot.emd2(np.ones(in_sample_size) / in_sample_size, np.ones(in_sample_size) / in_sample_size,\n",
    "                                     min_obj.detach().cpu().numpy())\n",
    "            else:\n",
    "                val = target_net(torch.ones(x_batch.shape[0], 1, device=device)*(time+1.0), x_batch, y_batch).reshape(-1)\n",
    "                min_obj = (l2_mat + DISCOUNT*val).reshape(in_sample_size, in_sample_size)\n",
    "                expected_v = ot.emd2(np.ones(in_sample_size)/in_sample_size, np.ones(in_sample_size)/in_sample_size,\n",
    "                                     min_obj.detach().cpu().numpy())\n",
    "\n",
    "            memory.push(torch.tensor([time], dtype=torch.float32, device=device), x_path_pool[smp_id, time, :],\n",
    "                        y_path_pool[smp_id, time, :], torch.tensor([expected_v], device=device))\n",
    "\n",
    "        # Optimize at time t\n",
    "        for opt_step in range(N_OPT):\n",
    "            loss = optimize_model(policy_net, memory, optimizer, Trunc_flag)\n",
    "            if Trunc_flag:\n",
    "                with torch.no_grad():\n",
    "                    for param in policy_net.parameters():\n",
    "                        ## param.add_(torch.randn(param.size(), device=device)/50)\n",
    "                        param.clamp_(-1.0, 1.0)\n",
    "            if loss:\n",
    "                loss_hist[time] += loss.detach().cpu().item()\n",
    "\n",
    "\n",
    "        loss_hist[time] /= N_OPT\n",
    "\n",
    "        # update target network\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        # test initial value\n",
    "        val = target_net(torch.ones(1, 1, device=device)*0.0, x_path_pool[0, 0, :].reshape(1, x_dim),\n",
    "                         y_path_pool[0, 0, :].reshape(1, y_dim)).reshape(-1)\n",
    "        val_hist[time] = val\n",
    "\n",
    "        # empty memory\n",
    "        memory.clear()\n",
    "        print('Time step', time, 'Loss', loss_hist[time])\n",
    "\n",
    "        # print('Shift vector in the last layer:', target_net.linear3.bias.sum().item())\n",
    "\n",
    "\n",
    "    # for name, param in target_net.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name, param.data)\n",
    "\n",
    "\n",
    "    print('Instance', n_ins)\n",
    "    # print('Time elapsed', end - start)\n",
    "    print('Last values', val_hist[0])\n",
    "    final_result[n_ins] = val_hist[0]\n",
    "\n",
    "print('All final value:', final_result)\n",
    "print('Final mean:', final_result.mean())\n",
    "print('Final std:', final_result.std())\n",
    "end = Clock.time()\n",
    "print('Average time for one instance:', (end-start)/N_INSTANCE)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(val_hist)\n",
    "# plt.xlabel('Steps', fontsize=16)\n",
    "# plt.ylabel(r'$V_0$', fontsize=16)\n",
    "# # plt.tick_params(axis = 'both', which = 'major', labelsize = 16)\n",
    "# plt.legend(bbox_to_anchor=(1, 1), title='', fontsize=16, title_fontsize=16)\n",
    "# plt.savefig('conti_val.pdf', format='pdf', dpi=1000, bbox_inches='tight', pad_inches=0.1)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(loss_hist)\n",
    "# plt.xlabel('Steps', fontsize=16)\n",
    "# plt.ylabel('Loss', fontsize=16)\n",
    "# plt.savefig('conti_loss.pdf', format='pdf', dpi=1000, bbox_inches='tight', pad_inches=0.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
